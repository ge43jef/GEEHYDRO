{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ge43jef/GEEHYDRO/blob/block3/machine_learning_base_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe892cc",
      "metadata": {
        "id": "6fe892cc"
      },
      "source": [
        "# Machine learning basics 2\n",
        "\n",
        "# Outline\n",
        "- [1 Multiple Variable Linear Regression](#toc_40015_1)\n",
        "- [2 Polynomial Regression](#toc_40015_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cfead2",
      "metadata": {
        "id": "84cfead2"
      },
      "source": [
        "<a name=\"toc_40015_1\"></a>\n",
        "## 1 Multiple Variable Linear Regression\n",
        "In this lab, you will:\n",
        "- extend our regression model routines to support multiple features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0825da1b",
      "metadata": {
        "id": "0825da1b"
      },
      "outputs": [],
      "source": [
        "import copy, math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d701821",
      "metadata": {
        "id": "4d701821"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "You will use the motivating example of actual evaporation rate  prediction. The training dataset contains three examples with four features (temperature, wind speed, relative humidity and pressure) shown in the table below.\n",
        "\n",
        "| temperature (degree) | wind speed(m/s)  | relative humidity | air pressure(kPa) | evaporation rate (mm/d)  |\n",
        "| ----------------| ------------------- |----------------- |--------------|-------------- |\n",
        "| 21.04            | 5                   | 0.5                | 90           | 46           |\n",
        "| 14.16            | 3                   | 1                | 80           | 23.2           |\n",
        "| 8.52             | 2                   | 0.5                | 70           | 17.8           |\n",
        "\n",
        "You will build a linear regression model using these values so you can then predict the evaporation rate under other air conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9ef3dd",
      "metadata": {
        "id": "5a9ef3dd"
      },
      "outputs": [],
      "source": [
        "X_train = np.array([[21.04,5,0.5,90], [14.16,3,1,80], [8.52,2,0.5,70]])\n",
        "y_train = np.array([46, 23.2, 17.8])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ca8c0b",
      "metadata": {
        "id": "e9ca8c0b"
      },
      "source": [
        "## Matrix X containing our examples\n",
        "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
        "\n",
        "\n",
        "$$\\mathbf{X} =\n",
        "\\begin{pmatrix}\n",
        " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\\n",
        " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
        " \\cdots \\\\\n",
        " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "notation:\n",
        "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
        "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.\n",
        "\n",
        "Display the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47cea53c",
      "metadata": {
        "id": "47cea53c"
      },
      "outputs": [],
      "source": [
        "# data is stored in numpy array/matrix\n",
        "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
        "print(X_train)\n",
        "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940befaf",
      "metadata": {
        "id": "940befaf"
      },
      "source": [
        "## Parameter vector w, b\n",
        "\n",
        "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
        "  - Each element contains the parameter associated with one feature.\n",
        "  - in our dataset, n is 4.\n",
        "  - notionally, we draw this as a column vector\n",
        "\n",
        "$$\\mathbf{w} = \\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\cdots\\\\\n",
        "w_{n-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "* $b$ is a scalar parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1b6465",
      "metadata": {
        "id": "fc1b6465"
      },
      "outputs": [],
      "source": [
        "b_init = 785.1811367994083\n",
        "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062e239f",
      "metadata": {
        "id": "062e239f"
      },
      "source": [
        "## Model Prediction With Multiple Variables\n",
        "The model's prediction with multiple variables is given by the linear model:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
        "or in vector notation:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
        "where $\\cdot$ is a vector `dot product`\n",
        "\n",
        "To demonstrate the dot product, we will implement prediction using (1) and (2)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99cf0207",
      "metadata": {
        "id": "99cf0207"
      },
      "source": [
        "## Compute Cost With Multiple Variables\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$\n",
        "\n",
        "\n",
        "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99e73c6",
      "metadata": {
        "id": "d99e73c6"
      },
      "outputs": [],
      "source": [
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
        "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
        "    cost = cost / (2 * m)                      #scalar\n",
        "    #print(cost)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0339a8a4",
      "metadata": {
        "id": "0339a8a4"
      },
      "outputs": [],
      "source": [
        "# Compute and display cost using our pre-chosen optimal parameters.\n",
        "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
        "print(f'Cost at optimal w : {cost}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e33bd197",
      "metadata": {
        "id": "e33bd197"
      },
      "source": [
        "## Gradient Descent With Multiple Variables\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "\n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a011525a",
      "metadata": {
        "id": "a011525a"
      },
      "source": [
        "## Compute Gradient with Multiple Variables\n",
        "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
        "- outer loop over all m examples.\n",
        "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
        "    - in a second loop over all n features:\n",
        "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3725e557",
      "metadata": {
        "id": "3725e557"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_db, dj_dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111ee3e3",
      "metadata": {
        "id": "111ee3e3"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               ##None\n",
        "        b = b - alpha * dj_db               ##None\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f59e66",
      "metadata": {
        "id": "b2f59e66"
      },
      "source": [
        "**Try different learning rate by changing alpha value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27dffd3c",
      "metadata": {
        "id": "27dffd3c"
      },
      "outputs": [],
      "source": [
        "# initialize parameters\n",
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 0.\n",
        "# some gradient descent settings\n",
        "iterations = 5000\n",
        "alpha = 5.0e-8\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
        "m,_ = X_train.shape\n",
        "for i in range(m):\n",
        "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")\n",
        "\n",
        "# plot cost versus iteration\n",
        "fig, (ax1) = plt.subplots(1, 1, constrained_layout=True, figsize=(12, 4))\n",
        "ax1.plot(J_hist)\n",
        "\n",
        "ax1.set_title(\"Cost vs. iteration\")\n",
        "ax1.set_ylabel('Cost')\n",
        "ax1.set_xlabel('iteration step')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb278843",
      "metadata": {
        "id": "cb278843"
      },
      "source": [
        "<a name=\"toc_40015_2\"></a>\n",
        "## 2 Feature Engineering and Polynomial Regression\n",
        "In this lab, you will:\n",
        "- explore feature engineering and polynomial regression which allows you to use the machinery of linear regression to fit very complicated, even very non-linear functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e22c9",
      "metadata": {
        "id": "037e22c9"
      },
      "source": [
        "## Feature Engineering and Polynomial Regression Overview\n",
        "\n",
        "Out of the box, linear regression provides a means of building models of the form:\n",
        "$$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
        "What if your features/data are non-linear or are combinations of features? How can we use the machinery of linear regression to fit this curve? Recall, the 'machinery' we have is the ability to modify the parameters $\\mathbf{w}$, $\\mathbf{b}$ in (1) to 'fit' the equation to the training data. However, no amount of adjusting of $\\mathbf{w}$,$\\mathbf{b}$ in (1) will achieve a fit to a non-linear curve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d909e7",
      "metadata": {
        "id": "e1d909e7"
      },
      "source": [
        "## Polynomial Features\n",
        "\n",
        "Above we were considering a scenario where the data was non-linear. Let's try using what we know so far to fit a non-linear curve. We'll start with a simple quadratic: $y = 1+x^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3151b792",
      "metadata": {
        "id": "3151b792"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               ##None\n",
        "        b = b - alpha * dj_db               ##None\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "\n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69287bf2",
      "metadata": {
        "id": "69287bf2"
      },
      "outputs": [],
      "source": [
        "# create target data\n",
        "x = np.arange(0, 20, 1)\n",
        "y = 1 + x**2\n",
        "X = x.reshape(-1, 1)\n",
        "initial_w = 0.0\n",
        "initial_b = 0.0\n",
        "alpha = 5e-5\n",
        "iterations = 1000\n",
        "mw_final, b_final, J_hist = gradient_descent(X, y, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"no feature engineering\")\n",
        "plt.plot(x,X@mw_final + b_final, label=\"Predicted Value\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab3e5244",
      "metadata": {
        "id": "ab3e5244"
      },
      "source": [
        "Well, as expected, not a great fit. What is needed is something like $y= w_0x_0^2 + b$, or a **polynomial feature**.\n",
        "To accomplish this, you can modify the *input data* to *engineer* the needed features. If you swap the original data with a version that squares the $x$ value, then you can achieve $y= w_0x_0^2 + b$. Let's try it. Swap `X` for `X**2` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42086f2a",
      "metadata": {
        "id": "42086f2a"
      },
      "outputs": [],
      "source": [
        "# create target data\n",
        "x = np.arange(0, 20, 1)\n",
        "y = 1 + x**2\n",
        "\n",
        "# Engineer features\n",
        "X = x**2      #<-- added engineered feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a15d4d",
      "metadata": {
        "id": "24a15d4d"
      },
      "outputs": [],
      "source": [
        "X = X.reshape(-1, 1)  #X should be a 2-D Matrix\n",
        "model_w,model_b,J_hist = gradient_descent(X, y, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "\n",
        "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Added x**2 feature\")\n",
        "plt.plot(x, np.dot(X,model_w) + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1ad5f9",
      "metadata": {
        "id": "7e1ad5f9"
      },
      "source": [
        "## Selecting Features\n",
        "\n",
        "Above, we knew that an $x^2$ term was required. It may not always be obvious which features are required. One could add a variety of potential features to try and find the most useful. For example, what if we had instead tried : $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ ?\n",
        "\n",
        "Run the next cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b0cdd1",
      "metadata": {
        "id": "32b0cdd1"
      },
      "outputs": [],
      "source": [
        "# create target data\n",
        "x = np.arange(0, 20, 1)\n",
        "y = x**2\n",
        "\n",
        "# engineer features .\n",
        "X = np.c_[x, x**2, x**3]   #<-- added engineered feature\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e45ccc",
      "metadata": {
        "id": "b5e45ccc"
      },
      "outputs": [],
      "source": [
        "initial_w = np.array([ 0.1, 0.1, 0.1])\n",
        "initial_b = 0.0\n",
        "alpha = 5e-10\n",
        "model_w,model_b,J_hist = gradient_descent(X, y, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "\n",
        "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"x, x**2, x**3 features\")\n",
        "plt.plot(x, X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d00ee44",
      "metadata": {
        "id": "0d00ee44"
      },
      "source": [
        "Try a larger learning rate, to see what will happen.\n",
        "What is the possible solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0feb980",
      "metadata": {
        "id": "d0feb980"
      },
      "source": [
        "Note the value of $\\mathbf{w}$, `[0.10 0.10 0.05]` and b is `-3.94`.This implies the model after fitting/training is:\n",
        "$$ 0.10x + 0.10x^2 + 0.05x^3 - 3.94 $$\n",
        "Gradient descent has emphasized the data that is the best fit to the $x^2$ data by increasing the $w_1$ term relative to the others.  If you were to run for a very long time, it would continue to reduce the impact of the other terms.\n",
        ">Gradient descent is picking the 'correct' features for us by emphasizing its associated parameter\n",
        "\n",
        "Let's review this idea:\n",
        "- Intially, the features were re-scaled so they are comparable to each other\n",
        "- less weight value implies less important/correct feature, and in extreme, when the weight becomes zero or very close to zero, the associated feature is not useful in fitting the model to the data.\n",
        "- above, after fitting, the weight associated with the $x^2$ feature is much larger than the weights for $x$ or $x^3$ as it is the most useful in fitting the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63be85af",
      "metadata": {
        "id": "63be85af"
      },
      "source": [
        "## An Alternate View\n",
        "Above, polynomial features were chosen based on how well they matched the target data. Another way to think about this is to note that we are still using linear regression once we have created new features. Given that, the best features will be linear relative to the target. This is best understood with an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e737a37d",
      "metadata": {
        "id": "e737a37d"
      },
      "outputs": [],
      "source": [
        "# create target data\n",
        "x = np.arange(0, 20, 1)\n",
        "y = x**2\n",
        "\n",
        "# engineer features .\n",
        "X = np.c_[x, x**2, x**3]   #<-- added engineered feature\n",
        "X_features = ['x','x^2','x^3']\n",
        "fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
        "for i in range(len(ax)):\n",
        "    ax[i].scatter(X[:,i],y)\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"y\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ed359a",
      "metadata": {
        "id": "31ed359a"
      },
      "source": [
        "Above, it is clear that the $x^2$ feature mapped against the target value $y$ is linear. Linear regression can then easily generate a model using that feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3029ea",
      "metadata": {
        "id": "1e3029ea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}