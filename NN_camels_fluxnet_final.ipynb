{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ge43jef/GEEHYDRO/blob/block5/NN_camels_fluxnet_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242fd350",
      "metadata": {
        "id": "242fd350"
      },
      "source": [
        "# Neural Network with pytorch\n",
        "In this lab, you will learn how to implement a neural network simple, we will use a library called pytorch to bulid up the neural network we did last lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XBUySHWFrtnP",
      "metadata": {
        "id": "XBUySHWFrtnP"
      },
      "outputs": [],
      "source": [
        "#pip install -U torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c9b022",
      "metadata": {
        "id": "22c9b022"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551e94ae",
      "metadata": {
        "id": "551e94ae"
      },
      "source": [
        "## B1. Define your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c12eeddf",
      "metadata": {
        "id": "c12eeddf"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Prepare the dataset for neural network\n",
        "    '''\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "            # Apply scaling if necessary\n",
        "\n",
        "            self.X = torch.from_numpy(X).type(torch.float)\n",
        "            self.y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64052fa3",
      "metadata": {
        "id": "64052fa3"
      },
      "source": [
        "## B2. NN Binary classification (example data of last lab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600c2aa4",
      "metadata": {
        "id": "600c2aa4"
      },
      "source": [
        "### B2.1 Data creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973ea466",
      "metadata": {
        "id": "973ea466"
      },
      "outputs": [],
      "source": [
        "X = np.array([[21.04,5,0.5,90], [14.16,3,1,80], [8.52,2,0.5,70\n",
        "], [7.52,2.3,1,80]])\n",
        "y = np.array([0, 0, 1 , 1 ])\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71371397",
      "metadata": {
        "id": "71371397"
      },
      "source": [
        "### B2.2 Define the NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f16549e",
      "metadata": {
        "id": "4f16549e"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(4, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(5, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a621f36d",
      "metadata": {
        "id": "a621f36d"
      },
      "source": [
        "### B2.3 Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee62f1c",
      "metadata": {
        "id": "5ee62f1c"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    dataset = Dataset(X.T , y)\n",
        "    trainloader = DataLoader(dataset, batch_size = 4)\n",
        "    mlp = Network()\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_function = nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(mlp.parameters(), lr=0.001)\n",
        "\n",
        "    # Run the training loop\n",
        "    for epoch in range(0, 100):\n",
        "\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch + 1}')\n",
        "\n",
        "        # Set current loss value\n",
        "        current_loss = 0.0\n",
        "\n",
        "        # Iterate over the DataLoader for training data\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # Get and prepare inputs\n",
        "            inputs, targets = data\n",
        "            targets = targets.reshape(targets.shape[0] , 1)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mlp(inputs)\n",
        "\n",
        "            loss = loss_function( outputs , targets )\n",
        "\n",
        "            # Perform backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            current_loss += loss.item()\n",
        "            if (i+1) % 1 == 0:\n",
        "                print('Loss after mini-batch %5d: %.3f' %\n",
        "                      (i + 1, loss.item()))\n",
        "                current_loss = 0.0\n",
        "\n",
        "\n",
        "    # Process is complete.\n",
        "    #print('Training process has finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55655840",
      "metadata": {
        "id": "55655840"
      },
      "source": [
        "### Please check the [reference](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html) for the accuracy calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500cc544",
      "metadata": {
        "id": "500cc544"
      },
      "source": [
        "## B3. NN Multiple classification (camel data of last lab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bbc5af",
      "metadata": {
        "id": "12bbc5af"
      },
      "source": [
        "### B3.1 Model define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b539e90",
      "metadata": {
        "id": "6b539e90"
      },
      "outputs": [],
      "source": [
        "class MLP_class(nn.Module):\n",
        "    '''\n",
        "      Multilayer Perceptron for classification.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(2, 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 4),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6723bbf2",
      "metadata": {
        "id": "6723bbf2"
      },
      "source": [
        "### B3.2 Load the dataset\n",
        "In this part, we will use camels dataset same as we used in the previous lab to perform the classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d6058b",
      "metadata": {
        "id": "d6d6058b"
      },
      "outputs": [],
      "source": [
        "with open('camels_topo.txt') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "num_of_rows = len(lines)\n",
        "\n",
        "var = np.zeros((num_of_rows - 1 , 4)) # save first fourth variables\n",
        "                                      # in the files\n",
        "\n",
        "for num in range(1 , num_of_rows): # we don't need the first row\n",
        "    xx = lines[num] # variable to save each line of lines\n",
        "    l = []\n",
        "    for t in xx.split(';'):\n",
        "        try:\n",
        "            l.append(float(t))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    var[num - 1 , :] = l[0 : 4]\n",
        "\n",
        "var = pd.DataFrame(var , columns = ['catchment_idx','lat','lon','elev'])\n",
        "\n",
        "var['elev_class'] , range_of_quantile = pd.qcut(var['elev'], 4, labels=False , retbins=True)\n",
        "var = np.array(var)\n",
        "\n",
        "X = var[: , [2,1]]\n",
        "y = var[: , 4]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059c06b8",
      "metadata": {
        "id": "059c06b8"
      },
      "source": [
        "### B3.3 Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e04e4bf7",
      "metadata": {
        "id": "e04e4bf7"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(52)\n",
        "    dataset = Dataset(X_train, y_train)\n",
        "    trainloader = DataLoader(dataset, batch_size = 100)\n",
        "    mlp = MLP_class()\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(mlp.parameters(), lr = 0.001)\n",
        "\n",
        "    # Run the training loop\n",
        "    for epoch in range(0, 1000):\n",
        "\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch + 1}')\n",
        "\n",
        "        # Set current loss value\n",
        "        current_loss = 0.0\n",
        "\n",
        "        # Iterate over the DataLoader for training data\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # Get and prepare inputs\n",
        "            inputs, targets = data\n",
        "            targets = targets.type(torch.LongTensor)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mlp(inputs)\n",
        "            loss = loss_function( outputs , targets )\n",
        "\n",
        "            # Perform backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            current_loss += loss.item()\n",
        "            if (i+1) % 3 == 0:\n",
        "                print('Loss after mini-batch %5d: %.3f' %\n",
        "                      (i + 1, loss.item()))\n",
        "                current_loss = 0.0\n",
        "\n",
        "    # Process is complete.\n",
        "    print('Training process has finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ecf5da7",
      "metadata": {
        "id": "3ecf5da7"
      },
      "source": [
        "### B3.4 Model test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad03d8f9",
      "metadata": {
        "id": "ad03d8f9"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset(X_test, y_test)\n",
        "testloader = DataLoader(dataset, batch_size = 1000)\n",
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "         # Get and prepare inputs\n",
        "        inputs, targets = data\n",
        "        targets = targets.type(torch.LongTensor)\n",
        "        outputs = mlp(inputs)\n",
        "        output = m(outputs)\n",
        "        output = torch.argmax(output, dim=1) # convert the probabity to label index\n",
        "        accuracymetric = MulticlassAccuracy(num_classes=4)\n",
        "        accuracy =accuracymetric(output, targets)\n",
        "print(output.detach())\n",
        "print(targets)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e93db9",
      "metadata": {
        "id": "96e93db9"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 1 , figsize=(6, 10))\n",
        "plt.subplots_adjust(wspace=0.4, hspace=0.2)\n",
        "\n",
        "cmap = plt.cm.get_cmap('PiYG', 4)\n",
        "ax[0].scatter(X[: , 0] , X[: , 1])\n",
        "sc1 = ax[0].scatter(X_test[: , 0] , X_test[: , 1] , c=output, cmap=cmap, s=20, edgecolors=\"k\")\n",
        "ax[0].set_title(\"Our prediction on test data\")\n",
        "ax[1].set_title(\"True value\")\n",
        "sc2 = ax[1].scatter(X[: , 0] , X[: , 1] , c=y, cmap=cmap, s=20, edgecolors=\"k\")\n",
        "bounds = [0, 1, 2, 3, 4]\n",
        "plt.colorbar(sc1 , ticks=bounds , ax = ax[0])\n",
        "plt.colorbar(sc2 , ticks=bounds , ax = ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93442d0",
      "metadata": {
        "id": "b93442d0"
      },
      "source": [
        "## B4. NN for regression (FLUXNET data of previous lab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851be6d9",
      "metadata": {
        "id": "851be6d9"
      },
      "outputs": [],
      "source": [
        "def normalization(x):\n",
        "    x= (x-min(x)) / (max(x) - min(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02917c51",
      "metadata": {
        "id": "02917c51"
      },
      "outputs": [],
      "source": [
        "class MLP_regree(nn.Module):\n",
        "    '''\n",
        "      Multilayer Perceptron for classification.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(6, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 6),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(6, 1),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55e95bf",
      "metadata": {
        "id": "a55e95bf"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('FLX_US-Ne1_FLUXNET2015_SUBSET_DD_2001-2013_1-4.csv' , delimiter=\",\", skipinitialspace=True,  parse_dates=True)\n",
        "\n",
        "meteo = pd.DataFrame(\n",
        "            {\"sw\": data.SW_IN_F, \"lw\": data.LW_IN_F, \"tmp\": data.TA_F,\n",
        "             \"pre\": data.PA_F, \"u10\": data.WS_F,  \"vpd\": data.VPD_F , \"lh\": data.LE_CORR})\n",
        "\n",
        "data_all = np.array(meteo)\n",
        "\n",
        "for i in np.arange(7):\n",
        "    data_all[: , i] = normalization(data_all[:,i])\n",
        "\n",
        "\n",
        "X = data_all[ : , 0:6]\n",
        "y = data_all[ : , 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eda39e3",
      "metadata": {
        "id": "0eda39e3"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e1e60a",
      "metadata": {
        "id": "37e1e60a"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(4078)\n",
        "    dataset = Dataset(X_train, y_train)\n",
        "    trainloader = DataLoader(dataset, batch_size = 500 , shuffle=True)\n",
        "    mlp = MLP_regree()\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(mlp.parameters(), lr = 0.0001)\n",
        "\n",
        "    # Run the training loop\n",
        "    for epoch in range(0, 1000):\n",
        "\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch + 1}')\n",
        "\n",
        "        # Set current loss value\n",
        "        current_loss = 0.0\n",
        "\n",
        "        # Iterate over the DataLoader for training data\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # Get and prepare inputs\n",
        "            inputs, targets = data\n",
        "            #inputs, targets = inputs.float(), targets.float()\n",
        "            targets = targets.reshape((targets.shape[0], 1))\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mlp(inputs)\n",
        "            loss = loss_function(outputs , targets )\n",
        "\n",
        "            # Perform backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            current_loss += loss.item()\n",
        "            if (i+1) % 3 == 0:\n",
        "                print('Loss after mini-batch %5d: %.3f' %\n",
        "                      (i + 1, loss.item()))\n",
        "                current_loss = 0.0\n",
        "\n",
        "    # Process is complete.\n",
        "    print('Training process has finished.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb727ed",
      "metadata": {
        "id": "ceb727ed"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset(X_test, y_test)\n",
        "testloader = DataLoader(dataset, batch_size = len(dataset))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "         # Get and prepare inputs\n",
        "        inputs, targets = data\n",
        "        output = mlp(inputs)\n",
        "        output = torch.reshape(output, (-1,))\n",
        "        loss = loss_function(output,targets)\n",
        "\n",
        "print('MSE: '+ str(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50cffba5",
      "metadata": {
        "id": "50cffba5"
      },
      "outputs": [],
      "source": [
        "# plot the data\n",
        "# Plot the data points\n",
        "fig = plt.figure()\n",
        "fig,ax=plt.subplots(2, 1, figsize=(6, 12), sharey=True)\n",
        "ax[0].plot( y_test, marker='x', c='r',label='True Value')\n",
        "ax[0].plot( output , c='b',label='Our Prediction on test data')\n",
        "ax[0].set(xlabel=\"time (day)\", ylabel=\"evaporation rate (normalized)\")\n",
        "ax[1].scatter( y_test , output.flatten() , c='b')\n",
        "z = np.polyfit(y_test , output.flatten() , 1)\n",
        "y_hat = np.poly1d(z)(output)\n",
        "plt.plot(output.flatten(), y_hat, \"r--\", lw=2)\n",
        "text = f\"$y={z[0]:0.3f}\\;x{z[1]:+0.3f}$\\n$R^2 = {r2_score(y_test, y_hat):0.3f}$\\n\" \\\n",
        "                   f\"$RMSE = {mean_squared_error(y_test, y_hat, squared=False):0.3f} $ \"\n",
        "plt.gca().text(0.05, 0.95, text, transform=plt.gca().transAxes,\n",
        "                           fontsize=14, verticalalignment='top')\n",
        "plt.ylabel('Predict Value')\n",
        "# Set the x-axis label\n",
        "plt.xlabel('True Value')\n",
        "ax[0].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d9ae03",
      "metadata": {
        "id": "28d9ae03"
      },
      "source": [
        "### B5. Final case\n",
        "### Now let us implement a complete deep learning project involving several standard steps:\n",
        "B5.1 feature scaling\\\n",
        "B5.2 hyperparameter of neural network\\\n",
        "B5.3 cross validation\\\n",
        "B5.4 final model optimization\\\n",
        "B5.5 visualization\\"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0c0874",
      "metadata": {
        "id": "be0c0874"
      },
      "source": [
        "### B5.1. Feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e03932",
      "metadata": {
        "id": "01e03932"
      },
      "outputs": [],
      "source": [
        "def normalization(x):\n",
        "    x= (x-min(x)) / (max(x) - min(x))\n",
        "    return x\n",
        "\n",
        "data = pd.read_csv('FLX_US-Ne1_FLUXNET2015_SUBSET_DD_2001-2013_1-4.csv' , delimiter=\",\", skipinitialspace=True,  parse_dates=True)\n",
        "\n",
        "meteo = pd.DataFrame(\n",
        "            {\"sw\": data.SW_IN_F, \"lw\": data.LW_IN_F, \"tmp\": data.TA_F,\n",
        "             \"pre\": data.PA_F, \"u10\": data.WS_F,  \"vpd\": data.VPD_F , \"lh\": data.LE_CORR})\n",
        "\n",
        "data_all = np.array(meteo)\n",
        "\n",
        "for i in np.arange(7):\n",
        "    data_all[: , i] = normalization(data_all[:,i])\n",
        "\n",
        "\n",
        "X = data_all[ : , 0:6]\n",
        "y = data_all[ : , 6]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d4aec1",
      "metadata": {
        "id": "60d4aec1"
      },
      "source": [
        "### B5.2. Hyperparameter of neural network\n",
        "Change the number of layers, and neuron numbers manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aea200c",
      "metadata": {
        "id": "2aea200c"
      },
      "outputs": [],
      "source": [
        "class MLP_regree(nn.Module):\n",
        "    '''\n",
        "      Multilayer Perceptron for classification.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(6, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 6),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(6, 1),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1c46c4",
      "metadata": {
        "id": "fe1c46c4"
      },
      "source": [
        "### B5.3. Cross validation\n",
        "Optimize or fine-tune more hyperparameter such as learning rate and batch size\n",
        "In your future work, you can use optimization algorithm such as \"grid search\" for hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d52518",
      "metadata": {
        "id": "95d52518"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(4078)\n",
        "    dataset = Dataset(X_train, y_train)\n",
        "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "\n",
        "        print(f'FOLD {fold}')\n",
        "        print('--------------------------------')\n",
        "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "        trainloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=100, sampler=train_subsampler )\n",
        "        testloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=100 , sampler=test_subsampler)\n",
        "        mlp = MLP_regree()\n",
        "        loss_function = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(mlp.parameters(), lr = 0.0001)\n",
        "\n",
        "    # Run the training loop\n",
        "        for epoch in range(0, 100):\n",
        "\n",
        "        # Print epoch\n",
        "            print(f'Starting epoch {epoch + 1}')\n",
        "\n",
        "        # Set current loss value\n",
        "            current_loss = 0.0\n",
        "\n",
        "        # Iterate over the DataLoader for training data\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "            # Get and prepare inputs\n",
        "                inputs, targets = data\n",
        "            #inputs, targets = inputs.float(), targets.float()\n",
        "                targets = targets.reshape((targets.shape[0], 1))\n",
        "\n",
        "            # Zero the gradients\n",
        "                optimizer.zero_grad()\n",
        "                outputs = mlp(inputs)\n",
        "                loss = loss_function( outputs , targets )\n",
        "\n",
        "            # Perform backward pass\n",
        "                loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "                optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "\n",
        "                if i % 10 == 0:\n",
        "                    print('Loss after mini-batch %5d: %.3f' %\n",
        "                      (i + 1, loss.item()))\n",
        "                current_loss = 0.0\n",
        "\n",
        "    # Process is complete.\n",
        "    print('Training process has finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4edf6924",
      "metadata": {
        "id": "4edf6924"
      },
      "source": [
        "### B5.4. Final model optimization\n",
        "Best hyperparameter combinations based on cross validation performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e586668a",
      "metadata": {
        "id": "e586668a"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    loss_values = []\n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(4078)\n",
        "    dataset = Dataset(X_train, y_train)\n",
        "    trainloader = DataLoader(dataset, batch_size = 500 , shuffle=True)\n",
        "    mlp = MLP_regree() # model with optimal hyperparameter combinations\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(mlp.parameters(), lr = 0.0001)\n",
        "\n",
        "    # Run the training loop\n",
        "    for epoch in range(0, 100):\n",
        "\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch + 1}')\n",
        "\n",
        "        # Set current loss value\n",
        "        current_loss = 0.0\n",
        "\n",
        "        # Iterate over the DataLoader for training data\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # Get and prepare inputs\n",
        "            inputs, targets = data\n",
        "            #inputs, targets = inputs.float(), targets.float()\n",
        "            targets = targets.reshape((targets.shape[0], 1))\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            outputs = mlp(inputs)\n",
        "            loss = loss_function(outputs , targets )\n",
        "\n",
        "            # Perform backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "            current_loss += loss.item()\n",
        "\n",
        "            # Print statistics\n",
        "            loss_values.append(current_loss / len(trainloader))\n",
        "            if i % 5 == 0:\n",
        "                print('Loss after mini-batch %5d: %.3f' %\n",
        "                      (i + 1, loss.item()))\n",
        "            current_loss = 0.0\n",
        "\n",
        "    # Process is complete.\n",
        "    print('Training process has finished.')\n",
        "    plt.plot(loss_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c29b25",
      "metadata": {
        "id": "f9c29b25"
      },
      "source": [
        "### B5.5. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d507cb",
      "metadata": {
        "id": "29d507cb"
      },
      "outputs": [],
      "source": [
        "# performance on test data\n",
        "dataset = Dataset(X_test, y_test)\n",
        "testloader = DataLoader(dataset, batch_size = 1000)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "         # Get and prepare inputs\n",
        "        inputs, targets = data\n",
        "        output = mlp(inputs)\n",
        "\n",
        "\n",
        "# Plot the data points\n",
        "fig = plt.figure()\n",
        "fig,ax=plt.subplots(2, 1, figsize=(6, 12), sharey=True)\n",
        "ax[0].plot( y_test, marker='x', c='r',label='True Value')\n",
        "ax[0].plot( output , c='b',label='Our Prediction on test data')\n",
        "ax[0].set(xlabel=\"time (day)\", ylabel=\"evaporation rate (normalized)\")\n",
        "ax[1].scatter( y_test , output.flatten() , c='b')\n",
        "z = np.polyfit(y_test , output.flatten() , 1)\n",
        "y_hat = np.poly1d(z)(output)\n",
        "plt.plot(output.flatten(), y_hat, \"r--\", lw=2)\n",
        "text = f\"$y={z[0]:0.3f}\\;x{z[1]:+0.3f}$\\n$R^2 = {r2_score(y_test, y_hat):0.3f}$\\n\" \\\n",
        "                   f\"$RMSE = {mean_squared_error(y_test, y_hat, squared=False):0.3f} $ \"\n",
        "plt.gca().text(0.05, 0.95, text, transform=plt.gca().transAxes,\n",
        "                           fontsize=14, verticalalignment='top')\n",
        "plt.ylabel('Predict Value')\n",
        "# Set the x-axis label\n",
        "plt.xlabel('True Value')\n",
        "ax[0].legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}